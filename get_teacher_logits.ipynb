{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher Model Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "import torch\n",
    "from math import floor\n",
    "from transformers import GitForCausalLM, GitConfig, BertTokenizer\n",
    "\n",
    "from generativeimage2text.torch_common import torch_load, load_state_dict\n",
    "from generativeimage2text.model import get_git_model\n",
    "from generativeimage2text.tsv_io import TSVFile, tsv_writer, tsv_reader\n",
    "from generativeimage2text.inference import get_image_transform\n",
    "\n",
    "param = {\"num_image_with_embedding\":15}\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "# image_tsv = TSVFile(image_tsv)\n",
    "transforms = get_image_transform(param)\n",
    "\n",
    "model = get_git_model(tokenizer, param)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "import torch\n",
    "from math import floor\n",
    "from transformers import GitForCausalLM, GitConfig, BertTokenizer\n",
    "\n",
    "from generativeimage2text.torch_common import torch_load, load_state_dict\n",
    "from generativeimage2text.model import get_git_model\n",
    "from generativeimage2text.tsv_io import TSVFile, tsv_writer, tsv_reader\n",
    "from generativeimage2text.inference import get_image_transform\n",
    "\n",
    "param = {\"num_image_with_embedding\":15}\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "# image_tsv = TSVFile(image_tsv)\n",
    "transforms = get_image_transform(param)\n",
    "\n",
    "model = get_git_model(tokenizer, param)\n",
    "pretrained = f'model/GIT_BASE_MSRVTT.pt'\n",
    "checkpoint = torch_load(pretrained)['model']\n",
    "load_state_dict(model, checkpoint)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "# model.config.num_image_with_embedding = 20\n",
    "\n",
    "# set seed for reproducability\n",
    "np.random.seed(45)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        # transfer to PIL image\n",
    "        frame = frame.to_image()\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    frames = [transforms(i) for i in frames]\n",
    "    # return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "    return frames\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    '''\n",
    "    Sample a given number of frame indices from the video.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    '''\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n",
    "\n",
    "# load video\n",
    "# file_path = hf_hub_download(\n",
    "#     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n",
    "# )\n",
    "file_path=\"../../../USC/CSCI567/project/datasets/MSRVTT/videos/all/video0.mp4\"\n",
    "\n",
    "container = av.open(file_path)\n",
    "\n",
    "# sample frames\n",
    "# model.config.num_image_with_embedding = 20\n",
    "num_frames = model.num_image_with_embedding\n",
    "indices = sample_frame_indices(\n",
    "    clip_len=num_frames, frame_sample_rate=4, seg_len=container.streams.video[0].frames\n",
    "    # clip_len=num_frames, frame_sample_rate=floor(container.streams.video[0].frames/num_frames), seg_len=container.streams.video[0].frames\n",
    ")\n",
    "frames = read_video_pyav(container, indices)\n",
    "frames = [i.unsqueeze(0).cuda() for i in frames]\n",
    "\n",
    "print(\"len(frames) = \", len(frames))\n",
    "print(\"frames[0].shape = \", frames[0].shape)\n",
    "# prefix\n",
    "max_text_len = 50\n",
    "prefix_encoding = tokenizer(\"\",\n",
    "                            padding='do_not_pad',\n",
    "                            truncation=True,\n",
    "                            add_special_tokens=False,\n",
    "                            max_length=max_text_len)\n",
    "payload = prefix_encoding['input_ids']\n",
    "if len(payload) > max_text_len - 2:\n",
    "    payload = payload[-(max_text_len - 2):]\n",
    "input_ids = [tokenizer.cls_token_id] + payload\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     features = model.get_image_feature({\n",
    "#         'image': frames,\n",
    "#         'prefix': torch.tensor(input_ids).unsqueeze(0).cuda(),\n",
    "#     })\n",
    "# print(\"features.shape = \",features.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    result = model({\n",
    "        'image': frames,\n",
    "        'prefix': torch.tensor(input_ids).unsqueeze(0).cuda(),\n",
    "    })\n",
    "cap = tokenizer.decode(result['predictions'][0].tolist(), skip_special_tokens=True)\n",
    "cap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logits extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm,trange\n",
    "from PIL import Image\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "import pickle\n",
    "from math import floor\n",
    "from transformers import GitForCausalLM, GitConfig, BertTokenizer\n",
    "\n",
    "from generativeimage2text.torch_common import torch_load, load_state_dict\n",
    "from generativeimage2text.model import get_git_model\n",
    "from generativeimage2text.tsv_io import TSVFile, tsv_writer, tsv_reader\n",
    "from generativeimage2text.inference import get_image_transform\n",
    "\n",
    "param = {\"num_image_with_embedding\":15}\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "# image_tsv = TSVFile(image_tsv)\n",
    "transforms = get_image_transform(param)\n",
    "\n",
    "model = get_git_model(tokenizer, param)\n",
    "pretrained = f'model/GIT_BASE_MSRVTT.pt'\n",
    "checkpoint = torch_load(pretrained)['model']\n",
    "load_state_dict(model, checkpoint)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "# model.config.num_image_with_embedding = 20\n",
    "\n",
    "\n",
    "num_frames = model.num_image_with_embedding\n",
    "\n",
    "# set seed for reproducability\n",
    "np.random.seed(45)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        # transfer to PIL image\n",
    "        frame = frame.to_image()\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    frames = [transforms(i) for i in frames]\n",
    "    # return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "    return frames\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    '''\n",
    "    Sample a given number of frame indices from the video.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    '''\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n",
    "\n",
    "# load video\n",
    "file_path=\"../../../USC/CSCI567/project/datasets/MSRVTT/videos/all/\"\n",
    "#file_path = \"dataset/MSRVTT/data/MSRVTT/videos/all\"\n",
    "max_text_len = 50\n",
    "prefix_encoding = tokenizer(\"\",\n",
    "                            padding='do_not_pad',\n",
    "                            truncation=True,\n",
    "                            add_special_tokens=False,\n",
    "                            max_length=max_text_len)\n",
    "payload = prefix_encoding['input_ids']\n",
    "if len(payload) > max_text_len - 2:\n",
    "    payload = payload[-(max_text_len - 2):]\n",
    "input_ids = [tokenizer.cls_token_id] + payload\n",
    "\n",
    "batch_size=1\n",
    "\n",
    "# video_logits_batch = torch.zeros((batch_size*4, 30522)).cuda()\n",
    "batch_count = 0\n",
    "#cur_idx = 0\n",
    "frames_batch_list = []\n",
    "for i in range(param['num_image_with_embedding']):\n",
    "    frames_batch_list.append(torch.empty((batch_size, 3, 224, 224)).cuda())\n",
    "\n",
    "filename_list=os.listdir(file_path)\n",
    "filename_list.sort()\n",
    "#random.shuffle(filename_list)\n",
    "video_name_list=[]\n",
    "cur_idx = 0\n",
    "\n",
    "save_path = \"/../mnt/e/datasets/MSRVTT/video_logits_batch\"\n",
    "\n",
    "for filename in tqdm(filename_list[3000:4000]):\n",
    "    video_path = os.path.join(file_path, filename)\n",
    "    if not os.path.isfile(video_path) or not filename.endswith(('.mp4', '.avi', '.mkv')): \n",
    "        continue\n",
    "    video_path = os.path.join(file_path, filename)\n",
    "    container = av.open(video_path)\n",
    "    \n",
    "    if num_frames*4>=container.streams.video[0].frames:\n",
    "        # print('num-frame: ', container.streams.video[0].frames)\n",
    "        continue\n",
    "    video_name_list.append(filename)\n",
    "\n",
    "    indices = sample_frame_indices(\n",
    "        clip_len=num_frames, frame_sample_rate=4, seg_len=container.streams.video[0].frames\n",
    "    )\n",
    "    \n",
    "    frames = read_video_pyav(container, indices)\n",
    "    frames = [i.unsqueeze(0).cuda() for i in frames]\n",
    "    for i in range(param['num_image_with_embedding']):\n",
    "        frames_batch_list[i][batch_count, :, :, :] = frames[i]\n",
    "    container.close()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        result = model.get_video_logits({\n",
    "            'image': frames_batch_list,\n",
    "            'prefix': torch.tensor(input_ids).unsqueeze(0).cuda(),\n",
    "            # 'prefix': torch.tensor([input_ids]*batch_size).cuda(),\n",
    "        })\n",
    "    # video_logits_batch = result[\"logits\"]\n",
    "\n",
    "    with open(f\"{save_path}/{filename}.pkl\", \"wb\") as f:\n",
    "        # print(video_feature_batch)\n",
    "        pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feacher extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "import pickle\n",
    "from math import floor\n",
    "from transformers import GitForCausalLM, GitConfig, BertTokenizer\n",
    "\n",
    "from generativeimage2text.torch_common import torch_load, load_state_dict\n",
    "from generativeimage2text.model import get_git_model\n",
    "from generativeimage2text.tsv_io import TSVFile, tsv_writer, tsv_reader\n",
    "from generativeimage2text.inference import get_image_transform\n",
    "\n",
    "param = {\"num_image_with_embedding\":15}\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "# image_tsv = TSVFile(image_tsv)\n",
    "transforms = get_image_transform(param)\n",
    "\n",
    "model = get_git_model(tokenizer, param)\n",
    "pretrained = f'model/GIT_BASE_MSRVTT.pt'\n",
    "checkpoint = torch_load(pretrained)['model']\n",
    "load_state_dict(model, checkpoint)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "# model.config.num_image_with_embedding = 20\n",
    "\n",
    "\n",
    "num_frames = model.num_image_with_embedding\n",
    "\n",
    "# set seed for reproducability\n",
    "np.random.seed(45)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        # transfer to PIL image\n",
    "        frame = frame.to_image()\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    frames = [transforms(i) for i in frames]\n",
    "    # return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "    return frames\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    '''\n",
    "    Sample a given number of frame indices from the video.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    '''\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n",
    "\n",
    "# load video\n",
    "file_path=\"../../../USC/CSCI567/project/datasets/MSRVTT/videos/all/\"\n",
    "#file_path = \"dataset/MSRVTT/data/MSRVTT/videos/all\"\n",
    "max_text_len = 50\n",
    "prefix_encoding = tokenizer(\"\",\n",
    "                            padding='do_not_pad',\n",
    "                            truncation=True,\n",
    "                            add_special_tokens=False,\n",
    "                            max_length=max_text_len)\n",
    "payload = prefix_encoding['input_ids']\n",
    "if len(payload) > max_text_len - 2:\n",
    "    payload = payload[-(max_text_len - 2):]\n",
    "input_ids = [tokenizer.cls_token_id] + payload\n",
    "\n",
    "batch_size=20\n",
    "pca_video_size=500\n",
    "\n",
    "video_feature_batch = torch.zeros((pca_video_size, 197*param['num_image_with_embedding'], 768)).cuda()\n",
    "batch_count = 0\n",
    "#cur_idx = 0\n",
    "frames_batch_list = []\n",
    "for i in range(param['num_image_with_embedding']):\n",
    "    frames_batch_list.append(torch.empty((batch_size, 3, 224, 224)).cuda())\n",
    "\n",
    "filename_list=os.listdir(file_path)\n",
    "#random.shuffle(filename_list)\n",
    "video_name_list=[]\n",
    "cur_idx = 0\n",
    "print(filename_list[cur_idx])\n",
    "\n",
    "save_path = \"/../mnt/e/datasets/MSRVTT/video_feature_batch\"\n",
    "cur_idx = 0\n",
    "for cur_idx in range(int(10000/pca_video_size)):\n",
    "    filename_list_100 = filename_list[cur_idx*pca_video_size:cur_idx*pca_video_size+pca_video_size]\n",
    "    vid_batch_idx = 0\n",
    "    batch_count = 0\n",
    "    for filename in tqdm(filename_list_100):\n",
    "        if vid_batch_idx>=pca_video_size:\n",
    "            break\n",
    "        video_path = os.path.join(file_path, filename)\n",
    "        if not os.path.isfile(video_path) or not filename.endswith(('.mp4', '.avi', '.mkv')): \n",
    "            continue\n",
    "        if batch_count<batch_size:\n",
    "            video_path = os.path.join(file_path, filename)\n",
    "            container = av.open(video_path)\n",
    "            # print(container.name)\n",
    "            # sample frames\n",
    "            if num_frames*4>=container.streams.video[0].frames:\n",
    "                print('num-frame: ', container.streams.video[0].frames)\n",
    "                video_feature_batch[vid_batch_idx-batch_size+1:vid_batch_idx+1,:,:] = torch.zeros((batch_size, 197*param['num_image_with_embedding'], 768)).cuda()\n",
    "                continue\n",
    "            video_name_list.append(filename)\n",
    "\n",
    "            indices = sample_frame_indices(\n",
    "                clip_len=num_frames, frame_sample_rate=4, seg_len=container.streams.video[0].frames\n",
    "            )\n",
    "            \n",
    "            frames = read_video_pyav(container, indices)\n",
    "            frames = [i.unsqueeze(0).cuda() for i in frames]\n",
    "            for i in range(param['num_image_with_embedding']):\n",
    "                frames_batch_list[i][batch_count, :, :, :] = frames[i]\n",
    "            container.close()\n",
    "\n",
    "            if batch_count==batch_size-1:\n",
    "                #print('bc:',batch_count, vid_batch_idx, cur_idx)\n",
    "                with torch.no_grad():\n",
    "                    features_batch = model.get_image_feature({\n",
    "                        'image': frames_batch_list,\n",
    "                        'prefix': torch.tensor(input_ids).unsqueeze(0).cuda(),\n",
    "                    })\n",
    "                #print(features_batch.shape)\n",
    "                #print(cur_idx)\n",
    "                video_feature_batch[vid_batch_idx-batch_size+1:vid_batch_idx+1,:,:] = features_batch\n",
    "                #print(features_batch[-1,0,0:10])\n",
    "                #print(video_feature_batch[cur_idx,0,0:10])\n",
    "                batch_count=-1\n",
    "\n",
    "        batch_count+=1\n",
    "        vid_batch_idx+=1\n",
    "        \n",
    "    print(\"haha\")\n",
    "    with open(f\"{save_path}/video_feature_batch_{cur_idx}.pkl\", \"wb\") as f:\n",
    "        #print(video_feature_batch)\n",
    "        pickle.dump(video_feature_batch.to(\"cpu\"), f)\n",
    "    with open(f\"{save_path}/video_name_batch_{cur_idx}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(video_name_list, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
